# EnigmaEval

Evaluation harness for EnigmaEval - a benchmark for testing AI systems on puzzle-solving tasks.

## ğŸš€ Quick Start

### 1. Installation

```bash
git clone https://github.com/justinphan3110cais/engimaeval-eval.git
cd engimaeval-eval
pip install -r requirements.txt
```

### 2. Setup Environment Variables

```bash
# Copy the environment template
cp env.example .env

# Edit .env and add your API keys
```

### 3. Setup Dataset

The EnigmaEval dataset is private and not included in this repository. 

```bash
# Copy the dataset pickle file to the data/ directory
cp /path/to/enigmaeval.pkl data/enigmaeval.pkl
```

### 4. Run Evaluation

```bash
python enigmaeval_eval.py \
  --model gpt-5-mini \
  --split all \
  --output_dir results/enigmaeval/ \
  --models_config configs/models.yaml \
  --max_concurrent 128
```

## ğŸ“Š Usage

### Command Line Arguments

```bash
python enigmaeval_eval.py \
  --model MODEL_NAME \               # Model alias from configs/models.yaml
  --split SPLIT_NAME \               # Dataset split (see below)
  --output_file OUTPUT_PATH \        # Path for results JSON
  --models_config CONFIG_PATH \      # Path to models config (default: configs/models.yaml)
  --max_concurrent N \               # Max concurrent requests (default: 4)
```

### Model Configuration

Models are configured in `configs/models.yaml`.

## ğŸ“ Repository Structure

```
engimaeval-eval/
â”œâ”€â”€ enigmaeval_eval.py           # Main evaluation script
â”œâ”€â”€ enigmaeval_utils.py          # Utility functions and data loading
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ models.yaml              # Model configurations
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ llm_agents.py            # LLM agent interface
â”œâ”€â”€ prompt_templates/            # Prompt templates for different puzzle types
â”‚   â”œâ”€â”€ standard_tips.txt
â”‚   â”œâ”€â”€ imageless_tips.txt
â”‚   â”œâ”€â”€ mit_tips.txt
â”‚   â””â”€â”€ plagiarism.txt
â”œâ”€â”€ data/                        # Dataset storage (gitignored)
â”‚   â””â”€â”€ enigmaeval.pkl           # Dataset file (not in repo - must be copied)
â”œâ”€â”€ results/                     # Evaluation results (gitignored)
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```
## ğŸ™ Citation

If you use EnigmaEval in your research, please cite:

```bibtex
@misc{wang2025enigmaevalbenchmarklongmultimodal,
      title={EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges}, 
      author={Clinton J. Wang and Dean Lee and Cristina Menghini and Johannes Mols and Jack Doughty and Adam Khoja and Jayson Lynch and Sean Hendryx and Summer Yue and Dan Hendrycks},
      year={2025},
      eprint={2502.08859},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.08859}, 
}
```

## ğŸ”— Related

- Main CAIS simple-evals repository: [simple-evals](https://github.com/centerforaisafety/simple-evals)
