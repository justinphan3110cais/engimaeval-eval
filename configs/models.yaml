grok-4:
  model: xai/grok-4

grok-4-fast-reasoning:
  model: xai/grok-4-fast-reasoning

claude-sonnet-4-5:
  model: anthropic/claude-sonnet-4-5
  generation_config:
    max_tokens: 40000
    use_cache: false  # this is for prompt caching on long context/multi turns evals (eg. swebench, terminal bench, machiavelli, textquests)
    vertexai: true
    thinking:
      type: enabled
      budget_tokens: 32000

claude-opus-4-5:
  model: anthropic/claude-opus-4-5
  generation_config:
    max_tokens: 40000
    use_cache: false
    vertexai: false
    thinking:
      type: enabled
      budget_tokens: 32000

gpt-5:
  model: openai/gpt-5
  generation_config:
    reasoning_effort: high

gpt-5-mini:
  model: openai/gpt-5-mini
  generation_config:
    reasoning_effort: high

gemini-2.5-pro:
  model: gemini/gemini-2.5-pro
  generation_config:
    reasoning_effort: high

gemini-2.5-flash-lite:
  model: gemini/gemini-2.5-flash-lite

deepseek-v3.2:
  model: openai/deepseek-reasoner
  generation_config:
    api_key_env: DEEPSEEK_API_KEY
    api_base_url: https://api.deepseek.com
    provider: deepseek

kimi-k2:
  model: openai/kimi-k2-thinking
  generation_config:
    api_key_env: MOONSHOT_API_KEY
    api_base_url: https://api.moonshot.ai/v1
    provider: moonshot

gemini-3-pro-preview:
  model: gemini/gemini-3-pro-preview
  generation_config:
    reasoning_effort: high

grok-4-1-fast:
  model: xai/grok-4-1-fast-reasoning